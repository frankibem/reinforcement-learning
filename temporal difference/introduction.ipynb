{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Temporal-Difference Learning\n",
    "Dynamic programming (DP) solutions to RL problems require a perfect model of the environment; Monte-Carlo (MC) do not require a model of the environment and instead learn from experience-sample sequences of states, actions and rewards which can be obtained from an actual or simulated interaction with an environment. Temporal-Difference learning (TD) is a combination of both - like MC, it learns directly from raw experience without a model and like DP, it updates its estimates based in part on other estimates without waiting for a final outcome.\n",
    "\n",
    "### 1. SARSA (On-Policy TD Control)\n",
    "Here, we are interested in learning the value of state-action pairs. Starting with an arbitrary policy, we update our estimates of the values using the following equation:\n",
    "$$Q(S_t, A_t) \\gets Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \\right] $$\n",
    "\n",
    "Where $Q(S_t, A_t)$ is the value of taking action A in state S, $\\alpha$ is the step-size and $\\gamma$ is the discount factor.\n",
    "\n",
    "An update is performed after every step from a nonterminal state. If $S_{t+1}$ is terminal, then $Q(S_{t+1}, A_{t+1})$ is defined as zero. The update rule uses elements of the tuple $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$ which make up a transition from one state-action pair to the next giving it the name SARSA. Here's the SARSA algorithm:\n",
    "\n",
    "The \"Windy Gridworld\" problem and its variants illustrate the use of Sarsa to learn a policy.\n",
    "\n",
    "### 2. Q-learning (Off-Policy TD Control)\n",
    "Q-learning is considered to be one of the early breakthroughs in RL. It uses the following update equation:\n",
    "$$Q(S_t, A_t) \\gets Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma max_a Q(S_t+1, a) - Q(S_t, A_t) \\right]$$\n",
    "\n",
    "The update is also performed after every step as in Sarsa. The \"Exit Building\" example uses a simplified version of Q-learning to learn a policy and the \"Cliff Walking' example illustrates illustrates the differences between the policies learned using Sarsa and Q-learning.\n",
    "\n",
    "\n",
    "* Difference between on-policy and off-policy methods: https://stats.stackexchange.com/questions/184657/difference-between-off-policy-and-on-policy-learning\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
