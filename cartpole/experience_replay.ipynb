{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import cntk\n",
    "from cntk import *\n",
    "from cntk.layers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Some comments about replay buffer implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer: \n",
    "    \"\"\"\n",
    "    Fixed capacity buffer implemented as circular queue\n",
    "    Transitions are stored as (s, a, r, s') tuples\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.samples = np.ndarray(capacity, dtype=object)\n",
    "        self.capacity = capacity\n",
    "        self.counter = 0\n",
    "        self.flag = False\n",
    "        \n",
    "    def size(self):\n",
    "        if self.flag:\n",
    "            return self.capacity\n",
    "        else:\n",
    "            return self.counter\n",
    "        \n",
    "    def add(self, sample):\n",
    "        self.samples[self.counter] = sample\n",
    "        \n",
    "        self.counter += 1\n",
    "        if self.counter >= self.capacity:\n",
    "            self.counter = 0\n",
    "            self.flag = True\n",
    "            \n",
    "    def sample(self, n):\n",
    "        n = min(n, self.size())\n",
    "        \n",
    "        size = self.size()\n",
    "        if size < self.capacity:\n",
    "            return np.random.choice(self.samples[:size], n, replace=False)\n",
    "        else:\n",
    "            return np.random.choice(self.samples, n, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class ErAgent:\n",
    "    def __init__(self, input_dim, output_dim, batch_size, gamma, buffer):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.buffer = buffer\n",
    "        self.epsilon = 1\n",
    "        \n",
    "        # Create the model\n",
    "        self.input_var = input(input_dim, np.float32)\n",
    "        self.output_var = input(output_dim, np.float32)\n",
    "\n",
    "        self.model = Sequential([\n",
    "            Dense(16, activation=relu, name='dense1'),\n",
    "            Dense(16, activation=relu, name='dense2'),\n",
    "            Dense(self.output_dim, name='z')\n",
    "        ])(self.input_var)\n",
    "\n",
    "        loss = reduce_mean(square(self.model - self.output_var), axis=0)\n",
    "\n",
    "        learning_rate = 0.0025\n",
    "        lr_schedule = learning_rate_schedule(learning_rate, UnitType.sample)\n",
    "        learner = sgd(self.model.parameters, lr_schedule)\n",
    "        self.trainer = Trainer(self.model, loss, learner)\n",
    "        \n",
    "    def update_epsilon(self, episode):\n",
    "        \"\"\"\n",
    "        Updates epsilon using exponential decay with the decay rate chosen such\n",
    "        that epsilon is 0.05 by episode 8000\n",
    "        \"\"\"\n",
    "        self.epsilon = max(math.exp(-3.74e-4 * episode), 0.05)\n",
    "        \n",
    "    def predict(self, s):\n",
    "        \"\"\"\n",
    "        Feeds a state through the model (our network) and obtains the values of each action\n",
    "        \"\"\"\n",
    "        return self.model.eval(s)\n",
    "        \n",
    "    def act(self, s):\n",
    "        \"\"\"\n",
    "        Selects an action using the epoch-greedy approach\n",
    "        \"\"\"\n",
    "        if random.random() > self.epsilon:\n",
    "            # Exploit (greedy)            \n",
    "            return np.argmax(self.predict(s))\n",
    "        else:\n",
    "            # Explore (random action)\n",
    "            return random.randint(0, self.output_dim - 1)\n",
    "        \n",
    "    def observe(self, sample):\n",
    "        \"\"\"\n",
    "        Adds a transition to the replay buffer\n",
    "        \"\"\"\n",
    "        self.buffer.add(sample)\n",
    "        \n",
    "    def replay(self):\n",
    "        \"\"\"\n",
    "        Samples a random minibatch of transitions from the replay buffer,\n",
    "        computes the expected return and uses them to perform a gradient descent step\n",
    "        \"\"\"\n",
    "        batch = self.buffer.sample(self.batch_size)\n",
    "        batch_len = batch.size\n",
    "        \n",
    "        no_state = np.zeros(self.input_dim)  # target for terminal state is r\n",
    "        states = np.array([obs[0] for obs in batch], dtype=np.float32)\n",
    "        states_ = np.array([no_state if obs[3] is None else obs[3] for obs in batch], dtype=np.float32)\n",
    "        \n",
    "        p = self.predict(states)  # value of start state\n",
    "        p_ = self.predict(states_)  # value of end state\n",
    "        \n",
    "        shape = np.hstack([batch_len, self.input_dim])\n",
    "        x = np.array(states, dtype=np.float32)  # will be inputs to network\n",
    "        y = np.array(p, dtype=np.float32)  # will be targets for network\n",
    "        \n",
    "        actions = [obs[1] for obs in batch]\n",
    "        rewards = np.array([obs[2] for obs in batch], dtype=np.float32)\n",
    "        terminal = np.invert([True if obs[3] is None else False for obs in batch]).astype(np.float32)\n",
    "        targets = rewards + terminal * self.gamma * np.amax(p_, axis=1)\n",
    "                \n",
    "        y[range(batch_len), actions] = targets         \n",
    "                \n",
    "        # Perform gradient descent\n",
    "        self.trainer.train_minibatch({self.input_var: x, self.output_var: y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def initialize_buffer(env, buffer):\n",
    "    \"\"\"\n",
    "    Initializes the replay buffer using experiences generated by taking random actions\n",
    "    \"\"\"\n",
    "    actions = env.action_space.n\n",
    "    s = env.reset()\n",
    "    \n",
    "    while buffer.size() < buffer.capacity:\n",
    "        a = random.randint(0, actions - 1)\n",
    "        s_, r, done, info = env.step(a)\n",
    "        \n",
    "        if done:  # terminal state\n",
    "            s_ = None\n",
    "        \n",
    "        buffer.add((s, a, r, s_))\n",
    "        \n",
    "        if done:\n",
    "            s = env.reset()\n",
    "        else:\n",
    "            s = s_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train(env, episodes, gamma, buffer_capacity, minibatch_size):\n",
    "    \"\"\"\n",
    "    param env: The gym environment to train with\n",
    "    param episodes: The number of episodes to train for\n",
    "    param gamma: The discount factor\n",
    "    param epsilon_fn: Function which returns epsilon for epoch-greedy learning\n",
    "    \"\"\"\n",
    "    input_dim = env.observation_space.shape\n",
    "    output_dim = env.action_space.n \n",
    "    \n",
    "    # Create buffer and initialize using random transitions\n",
    "    buffer = ReplayBuffer(buffer_capacity)\n",
    "    initialize_buffer(env, buffer)\n",
    "    \n",
    "    # Create agent\n",
    "    agent = ErAgent(input_dim, output_dim, minibatch_size, gamma, buffer)\n",
    "    \n",
    "    episode = 0\n",
    "    rewards = 0\n",
    "    episode_rewards = []\n",
    "    s = env.reset()\n",
    "    s = s.astype(np.float32)\n",
    "    \n",
    "    while episode < episodes:\n",
    "        # Select action using policy derived from Q (e-greedy) \n",
    "        a = agent.act(s)\n",
    "\n",
    "        # Take action and observe next state and reward\n",
    "        s_, r, done, info = env.step(a)\n",
    "\n",
    "        # Store transition in replay buffer\n",
    "        s_ = s_.astype(np.float32)\n",
    "        agent.observe((s, a, r, s_))\n",
    "        \n",
    "        # Sample random minibatch, compute y_i and perform gradient descent step\n",
    "        agent.replay()\n",
    "\n",
    "        s = s_\n",
    "        rewards += r\n",
    "\n",
    "        # Episode over, reset environment, update epsilon\n",
    "        if done:                \n",
    "            episode += 1\n",
    "            agent.update_epsilon(episode)\n",
    "            episode_rewards.append(rewards)\n",
    "\n",
    "            if episode % 200 == 0:\n",
    "                print('Episode {}, reward = {}'.format(episode, rewards))\n",
    "\n",
    "            s = env.reset()\n",
    "            s = s.astype(np.float32)\n",
    "            rewards = 0\n",
    "    return agent.model, episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "episodes = 10000\n",
    "buffer_capacity = 1\n",
    "minibatch_size = 1\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model1, rewards1 = train(env, episodes, gamma, buffer_capacity, minibatch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd.Series(rewards1).rolling(window=100).mean().plot(label='with experience replay')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def evaluate(env, model, episodes):\n",
    "    \"\"\"\n",
    "    Computes the average performance of the trained model over 'episodes' episodes\n",
    "    \"\"\"\n",
    "    episode = 0\n",
    "    rewards = 0\n",
    "    \n",
    "    while episode < episodes:\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            a = np.argmax(model.eval(s.astype(np.float32)))\n",
    "            s, r, done, info = env.step(a)\n",
    "            rewards += r\n",
    "        episode += 1\n",
    "    \n",
    "    return rewards / float(episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ave_er = evaluate(env, model1, 100)\n",
    "print('Average reward (with experience replay) = {}'.format(ave_er))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.plot(range(len(rewards1)), rewards1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
